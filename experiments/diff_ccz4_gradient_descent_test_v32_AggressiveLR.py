â€
â€#Â FILE:Â diff_ccz4_gradient_descent_test_v32_AggressiveLR.py
â€
â€importÂ jax
â€importÂ jax.numpyÂ asÂ jnp
â€fromÂ jaxÂ importÂ jit,Â grad,Â checkpoint
â€fromÂ jax.laxÂ importÂ fori_loop,Â cond
â€fromÂ jax.tree_utilÂ importÂ register_pytree_node_class,Â tree_mapÂ 
â€fromÂ functoolsÂ importÂ partial
â€importÂ time
â€
â€#Â =============================================================================
â€#Â ---Â 0.Â KONFIGURASIÂ DANÂ PARAMETERÂ ---
â€#Â =============================================================================
â€NÂ =Â 10Â Â Â Â Â Â Â Â Â Â 
â€dxÂ =Â 0.1
â€FIELD_SHAPEÂ =Â (N,Â N,Â N)
â€DIMÂ =Â 3
â€
â€@register_pytree_node_class
â€classÂ Params:
â€Â Â Â Â defÂ __init__(self,Â kappa1,Â kappa2,Â eta):
â€Â Â Â Â Â Â Â Â self.kappa1Â =Â jnp.asarray(kappa1,Â dtype=jnp.float32)
â€Â Â Â Â Â Â Â Â self.kappa2Â =Â jnp.asarray(kappa2,Â dtype=jnp.float32)
â€Â Â Â Â Â Â Â Â self.etaÂ =Â jnp.asarray(eta,Â dtype=jnp.float32)
â€Â Â Â Â Â Â Â Â 
â€Â Â Â Â defÂ replace(self,Â **kwargs):
â€Â Â Â Â Â Â Â Â returnÂ Params(
â€Â Â Â Â Â Â Â Â Â Â Â Â kwargs.get('kappa1',Â self.kappa1),
â€Â Â Â Â Â Â Â Â Â Â Â Â kwargs.get('kappa2',Â self.kappa2),
â€Â Â Â Â Â Â Â Â Â Â Â Â kwargs.get('eta',Â self.eta)
â€Â Â Â Â Â Â Â Â )
â€
â€Â Â Â Â defÂ tree_flatten(self):
â€Â Â Â Â Â Â Â Â returnÂ (self.kappa1,Â self.kappa2,Â self.eta),Â None
â€Â Â Â Â 
â€Â Â Â Â @classmethod
â€Â Â Â Â defÂ tree_unflatten(cls,Â aux_data,Â children):
â€Â Â Â Â Â Â Â Â returnÂ cls(*children)
â€
â€GLOBAL_DTÂ =Â 0.005
â€
â€PARAMSÂ =Â Params(kappa1=2.0,Â kappa2=1.0,Â eta=2.0)
â€
â€TEST_TIMESTEPSÂ =Â 200Â Â Â Â Â Â 
â€CHECKPOINT_INTERVALÂ =Â 5Â Â Â 
â€#Â KOREKSIÂ KRITIS:Â TingkatkanÂ LearningÂ RateÂ secaraÂ drastisÂ untukÂ mengatasiÂ gradienÂ 1e-18
â€LEARNING_RATEÂ =Â 1.0Â Â Â Â Â Â Â 
â€N_ITERATIONSÂ =Â 200Â Â Â Â Â Â Â Â 
â€
â€#Â =============================================================================
â€#Â ---Â OPTIMIZERÂ ADAMÂ DARIÂ JAX/OPTICSÂ ---
â€#Â =============================================================================
â€@register_pytree_node_class
â€classÂ AdamState:
â€Â Â Â Â defÂ __init__(self,Â count,Â m_eta,Â v_eta):
â€Â Â Â Â Â Â Â Â self.countÂ =Â count
â€Â Â Â Â Â Â Â Â self.m_etaÂ =Â m_eta
â€Â Â Â Â Â Â Â Â self.v_etaÂ =Â v_eta
â€Â Â Â Â Â Â Â Â 
â€Â Â Â Â defÂ replace(self,Â **kwargs):
â€Â Â Â Â Â Â Â Â Â Â Â Â returnÂ AdamState(
â€Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â kwargs.get('count',Â self.count),
â€Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â kwargs.get('m_eta',Â self.m_eta),
â€Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â kwargs.get('v_eta',Â self.v_eta)
â€Â Â Â Â Â Â Â Â Â Â Â Â )
â€
â€Â Â Â Â defÂ tree_flatten(self):
â€Â Â Â Â Â Â Â Â childrenÂ =Â (self.count,Â self.m_eta,Â self.v_eta)
â€Â Â Â Â Â Â Â Â returnÂ children,Â None
â€Â Â Â Â 
â€Â Â Â Â @classmethod
â€Â Â Â Â defÂ tree_unflatten(cls,Â aux_data,Â children):
â€Â Â Â Â Â Â Â Â returnÂ cls(*children)
â€
â€defÂ adam_init(params):
â€Â Â Â Â returnÂ AdamState(jnp.array(0),Â jnp.zeros_like(params.eta,Â dtype=jnp.float32),Â jnp.zeros_like(params.eta,Â dtype=jnp.float32))
â€
â€defÂ adam_update(i,Â grad_eta,Â state,Â learning_rate=1e-3,Â b1=0.9,Â b2=0.999,Â eps=1e-8):
â€Â Â Â Â #Â MenggunakanÂ learning_rateÂ yangÂ disupplyÂ olehÂ argumenÂ fungsiÂ luar
â€Â Â Â Â lr_valÂ =Â learning_rateÂ 
â€Â Â Â Â 
â€Â Â Â Â stateÂ =Â state.replace(count=state.countÂ +Â 1)
â€Â Â Â Â iÂ =Â state.count
â€Â Â Â Â 
â€Â Â Â Â mÂ =Â (b1Â *Â state.m_eta)Â +Â (1Â -Â b1)Â *Â grad_eta
â€Â Â Â Â vÂ =Â (b2Â *Â state.v_eta)Â +Â (1Â -Â b2)Â *Â (grad_etaÂ *Â grad_eta)
â€Â Â Â Â 
â€Â Â Â Â m_hatÂ =Â mÂ /Â (1Â -Â b1**i)
â€Â Â Â Â v_hatÂ =Â vÂ /Â (1Â -Â b2**i)
â€Â Â Â Â 
â€Â Â Â Â #Â MenggunakanÂ lr_valÂ yangÂ disupply
â€Â Â Â Â delta_etaÂ =Â lr_valÂ *Â m_hatÂ /Â (jnp.sqrt(v_hat)Â +Â eps)
â€Â Â Â Â 
â€Â Â Â Â new_stateÂ =Â AdamState(state.count,Â m,Â v)
â€Â Â Â Â returnÂ new_state,Â delta_etaÂ 
â€
â€#Â =============================================================================
â€#Â ---Â 1.Â CCZ4Â STATEÂ (PYTREE)Â &Â INITIALIZATIONÂ ---
â€#Â =============================================================================
â€@register_pytree_node_class
â€classÂ CCZ4State:
â€Â Â Â Â defÂ __init__(self,Â phi,Â chi,Â K,Â At,Â Gam,Â Theta,Â Z,Â alpha,Â beta):
â€Â Â Â Â Â Â Â Â self.phi,Â self.chi,Â self.K,Â self.At,Â self.Gam,Â self.Theta,Â self.Z,Â self.alpha,Â self.betaÂ =Â \
â€Â Â Â Â Â Â Â Â Â Â Â Â phi,Â chi,Â K,Â At,Â Gam,Â Theta,Â Z,Â alpha,Â beta
â€Â Â Â Â defÂ to_tuple(self):
â€Â Â Â Â Â Â Â Â returnÂ (self.phi,Â self.chi,Â self.K,Â self.At,Â self.Gam,Â self.Theta,Â self.Z,Â self.alpha,Â self.beta)
â€Â Â Â Â @classmethod
â€Â Â Â Â defÂ from_tuple(cls,Â t):
â€Â Â Â Â Â Â Â Â returnÂ cls(*t)
â€Â Â Â Â defÂ tree_flatten(self):
â€Â Â Â Â Â Â Â Â returnÂ self.to_tuple(),Â None
â€Â Â Â Â @classmethod
â€Â Â Â Â defÂ tree_unflatten(cls,Â aux_data,Â children):
â€Â Â Â Â Â Â Â Â returnÂ cls(*children)
â€
â€defÂ initialize_minkowski(N):
â€Â Â Â Â zerosÂ =Â jnp.zeros(FIELD_SHAPE,Â dtype=jnp.float32)
â€Â Â Â Â onesÂ =Â jnp.ones(FIELD_SHAPE,Â dtype=jnp.float32)
â€Â Â Â Â rÂ =Â jnp.linalg.norm(jnp.mgrid[:N,:N,:N]Â -Â N/2,Â axis=0)Â *Â dx
â€Â Â Â Â rÂ =Â jnp.asarray(r,Â dtype=jnp.float32)
â€Â Â Â Â alpha_initÂ =Â onesÂ -Â 0.01Â *Â jnp.exp(-r**2Â /Â 5.0)
â€Â Â Â Â perturbationÂ =Â 1e-2Â *Â jnp.exp(-r**2Â /Â 5.0)
â€Â Â Â Â K_initÂ =Â zerosÂ +Â perturbation
â€Â Â Â Â returnÂ CCZ4State(zeros,Â ones,Â K_init,Â jnp.zeros((5,)Â +Â FIELD_SHAPE,Â dtype=jnp.float32),
â€Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â jnp.zeros((3,)Â +Â FIELD_SHAPE,Â dtype=jnp.float32),Â zeros,Â jnp.zeros((3,)Â +Â FIELD_SHAPE,Â dtype=jnp.float32),
â€Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â alpha_init,Â jnp.zeros((3,)Â +Â FIELD_SHAPE,Â dtype=jnp.float32))
â€
â€#Â =============================================================================
â€#Â ---Â 2.Â ALGEBRAÂ TENSORÂ RIILÂ &Â CONSTRAINTSÂ ---
â€#Â =============================================================================
â€defÂ diff6(f,Â dx):
â€Â Â Â Â d_xÂ =Â jnp.roll(f,Â -1,Â axis=0)Â -Â jnp.roll(f,Â 1,Â axis=0)
â€Â Â Â Â d_yÂ =Â jnp.roll(f,Â -1,Â axis=1)Â -Â jnp.roll(f,Â 1,Â axis=1)
â€Â Â Â Â d_zÂ =Â jnp.roll(f,Â -1,Â axis=2)Â -Â jnp.roll(f,Â 1,Â axis=2)
â€Â Â Â Â returnÂ jnp.stack([d_x,Â d_y,Â d_z],Â axis=0)Â /Â (2.0Â *Â dx)
â€defÂ grad6(f):Â returnÂ diff6(f,Â dx)
â€defÂ advect(f,Â beta,Â d):
â€Â Â Â Â grad_fÂ =Â d(f)
â€Â Â Â Â returnÂ beta[0]*grad_f[0]Â +Â beta[1]*grad_f[1]Â +Â beta[2]*grad_f[2]
â€
â€@jit
â€defÂ voigt_to_tensor(A_voigt):
â€Â Â Â Â shapeÂ =Â A_voigt.shape[1:]
â€Â Â Â Â A_fullÂ =Â jnp.zeros((DIM,Â DIM)Â +Â shape,Â dtype=jnp.float32)
â€Â Â Â Â A_fullÂ =Â A_full.at[0,0].set(A_voigt[0])
â€Â Â Â Â A_fullÂ =Â A_full.at[1,1].set(A_voigt[3])
â€Â Â Â Â A_fullÂ =Â A_full.at[2,2].set(-A_voigt[0]Â -Â A_voigt[3])
â€Â Â Â Â A_fullÂ =Â A_full.at[0,1].set(A_voigt[1]);Â A_fullÂ =Â A_full.at[1,0].set(A_voigt[1])
â€Â Â Â Â A_fullÂ =Â A_full.at[0,2].set(A_voigt[2]);Â A_fullÂ =Â A_full.at[2,0].set(A_voigt[2])
â€Â Â Â Â A_fullÂ =Â A_full.at[1,2].set(A_voigt[4]);Â A_fullÂ =Â A_full.at[2,1].set(A_voigt[4])
â€Â Â Â Â returnÂ A_full
â€
â€@jit
â€defÂ get_gamma_inv(state):
â€Â Â Â Â returnÂ jnp.identity(DIM,Â dtype=jnp.float32)[:,Â :,Â None,Â None,Â None]Â *Â jnp.ones((DIM,Â DIM)Â +Â FIELD_SHAPE,Â dtype=jnp.float32)
â€
â€@jit
â€defÂ hamiltonian_constraint(state):
â€Â Â Â Â chiÂ =Â state.chi
â€Â Â Â Â At_fullÂ =Â voigt_to_tensor(state.At)
â€Â Â Â Â gamma_invÂ =Â get_gamma_inv(state)
â€Â Â Â Â At_sqÂ =Â jnp.einsum('ij...,Â kl...,Â ik...,Â jl...Â ->Â ...',
â€Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â At_full,Â At_full,Â gamma_inv,Â gamma_inv)
â€Â Â Â Â log_chiÂ =Â jnp.log(chiÂ +Â 1e-15)
â€Â Â Â Â d_log_chiÂ =Â grad6(log_chi)
â€Â Â Â Â grad_sq_sumÂ =Â jnp.sum(d_log_chiÂ *Â d_log_chi,Â axis=0)
â€Â Â Â Â R_chiÂ =Â -8Â *Â grad_sq_sumÂ 
â€Â Â Â Â HÂ =Â (R_chiÂ +Â state.K**2Â -Â At_sq)Â /Â chiÂ +Â 2.0Â *Â state.KÂ *Â state.ThetaÂ 
â€Â Â Â Â returnÂ H
â€
â€@jit
â€defÂ momentum_constraint(state):
â€Â Â Â Â At_fullÂ =Â voigt_to_tensor(state.At)
â€Â Â Â Â d_AtÂ =Â grad6(At_full)
â€Â Â Â Â MiÂ =Â jnp.einsum('jk...,Â kij...Â ->Â i...',
â€Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â get_gamma_inv(state),Â d_At)
â€Â Â Â Â returnÂ MiÂ *Â 0.1
â€
â€@jit
â€defÂ tensor_to_voigt_rhs(A_full):
â€Â Â Â Â """MengambilÂ tensorÂ (3,Â 3,Â N,Â N,Â N)Â danÂ mengembalikanÂ 5Â komponenÂ VoigtÂ (5,Â N,Â N,Â N)."""
â€Â Â Â Â returnÂ jnp.stack([
â€Â Â Â Â Â Â Â Â A_full[0,0],
â€Â Â Â Â Â Â Â Â A_full[0,1],
â€Â Â Â Â Â Â Â Â A_full[0,2],
â€Â Â Â Â Â Â Â Â A_full[1,1],
â€Â Â Â Â Â Â Â Â A_full[1,2]
â€Â Â Â Â ],Â axis=0)
â€
â€#Â =============================================================================
â€#Â ---Â 3.Â CCZ4Â RHSÂ (MASTERÂ EVOLUTIONÂ EQUATIONS)Â ---
â€#Â =============================================================================
â€defÂ ccz4_rhs(state_tuple,Â dx,Â params):
â€Â Â Â Â stateÂ =Â CCZ4State.from_tuple(state_tuple)
â€Â Â Â Â dÂ =Â lambdaÂ f:Â diff6(f,Â dx)
â€Â Â Â Â LieÂ =Â lambdaÂ f:Â advect(f,Â state.beta,Â d)
â€Â Â Â Â HÂ =Â hamiltonian_constraint(state)
â€Â Â Â Â MiÂ =Â momentum_constraint(state)
â€Â Â Â Â 
â€Â Â Â Â alphaÂ =Â state.alpha
â€Â Â Â Â KÂ =Â state.K
â€Â Â Â Â At_fullÂ =Â voigt_to_tensor(state.At)
â€Â Â Â Â 
â€Â Â Â Â dTheta_dtÂ =Â (
â€Â Â Â Â Â Â Â Â Lie(state.Theta)
â€Â Â Â Â Â Â Â Â +Â 0.5Â *Â alphaÂ *Â H
â€Â Â Â Â Â Â Â Â -Â alphaÂ *Â (2.0Â -Â params.kappa1)Â *Â KÂ *Â state.Theta
â€Â Â Â Â Â Â Â Â -Â params.kappa1Â *Â alphaÂ *Â state.Theta
â€Â Â Â Â )
â€Â Â Â Â dZ_dtÂ =Â (
â€Â Â Â Â Â Â Â Â Lie(state.Z)
â€Â Â Â Â Â Â Â Â +Â alphaÂ *Â (MiÂ -Â (2/3)Â *Â d(K)Â *Â state.Theta)
â€Â Â Â Â Â Â Â Â -Â params.kappa2Â *Â alphaÂ *Â state.ZÂ 
â€Â Â Â Â )
â€
â€Â Â Â Â dphi_dtÂ =Â jnp.zeros_like(state.phi)
â€Â Â Â Â dchi_dtÂ =Â Lie(state.chi)Â +Â (1.0Â /Â 3.0)Â *Â state.chiÂ *Â alphaÂ *Â K
â€Â Â Â Â dK_dtÂ =Â (
â€Â Â Â Â Â Â Â Â Lie(K)
â€Â Â Â Â Â Â Â Â +Â alphaÂ *Â (K**2Â /Â 3.0Â +Â HÂ /Â 2.0)
â€Â Â Â Â Â Â Â Â +Â alphaÂ *Â 0.0
â€Â Â Â Â )
â€
â€Â Â Â Â Ricc_proxyÂ =Â KÂ *Â KÂ *Â jnp.identity(DIM,Â dtype=jnp.float32)[:,Â :,Â None,Â None,Â None]Â *Â 0.1
â€Â Â Â Â dAt_dtÂ =Â (
â€Â Â Â Â Â Â Â Â Lie(At_full)
â€Â Â Â Â Â Â Â Â -Â alphaÂ *Â (Ricc_proxyÂ -Â KÂ *Â At_full)
â€Â Â Â Â Â Â Â Â -Â 2.0Â *Â alphaÂ *Â KÂ *Â At_fullÂ 
â€Â Â Â Â )
â€Â Â Â Â 
â€Â Â Â Â dAt_dt_voigtÂ =Â tensor_to_voigt_rhs(dAt_dt)
â€
â€Â Â Â Â dGam_dtÂ =Â Lie(state.Gam)Â -Â params.etaÂ *Â alphaÂ *Â state.GamÂ +Â alphaÂ *Â MiÂ 
â€Â Â Â Â dalpha_dtÂ =Â -2.0Â *Â alphaÂ *Â K
â€Â Â Â Â dbeta_dtÂ =Â 0.75Â *Â state.GamÂ -Â params.etaÂ *Â state.betaÂ 
â€
â€Â Â Â Â returnÂ (dphi_dt,Â dchi_dt,Â dK_dt,Â dAt_dt_voigt,Â dGam_dt,Â dTheta_dt,Â dZ_dt,Â dalpha_dt,Â dbeta_dt)
â€
â€
â€#Â =============================================================================
â€#Â ---Â 4.Â INTEGRATORÂ &Â STEPÂ FUNCTIONSÂ ---
â€#Â =============================================================================
â€defÂ rk4_step(state_tuple,Â rhs_fn,Â dt,Â dx,Â params):
â€Â Â Â Â 
â€Â Â Â Â defÂ add_scaled(state,Â k,Â scale):
â€Â Â Â Â Â Â Â Â returnÂ tree_map(lambdaÂ x,Â y:Â xÂ +Â scaleÂ *Â y,Â state,Â k)
â€
â€Â Â Â Â dt_fÂ =Â jnp.float32(dt)
â€Â Â Â Â 
â€Â Â Â Â k1Â =Â rhs_fn(state_tuple,Â dx,Â params)
â€Â Â Â Â k2Â =Â rhs_fn(add_scaled(state_tuple,Â k1,Â dt_fÂ *Â 0.5),Â dx,Â params)
â€Â Â Â Â k3Â =Â rhs_fn(add_scaled(state_tuple,Â k2,Â dt_fÂ *Â 0.5),Â dx,Â params)
â€Â Â Â Â k4Â =Â rhs_fn(add_scaled(state_tuple,Â k3,Â dt_f),Â dx,Â params)
â€Â Â Â Â 
â€Â Â Â Â final_stepÂ =Â tree_map(lambdaÂ k1_c,Â k2_c,Â k3_c,Â k4_c:Â (dt_fÂ /Â 6.0)Â *Â (k1_cÂ +Â 2*k2_cÂ +Â 2*k3_cÂ +Â k4_c),
â€Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â k1,Â k2,Â k3,Â k4)
â€Â Â Â Â 
â€Â Â Â Â returnÂ tree_map(lambdaÂ s,Â f:Â sÂ +Â f,Â state_tuple,Â final_step)
â€
â€defÂ ccz4_step(state_tuple,Â dt,Â dx,Â params):
â€Â Â Â Â returnÂ rk4_step(state_tuple,Â ccz4_rhs,Â dt,Â dx,Â params)
â€
â€checkpointed_stepÂ =Â checkpoint(ccz4_step)
â€
â€#Â =============================================================================
â€#Â ---Â 5.Â LOOPINGÂ (EVOLUTION)Â ---
â€#Â =============================================================================
â€defÂ raw_full_adjoint_evolution(initial_state_tuple,Â num_steps,Â dt,Â dx,Â params,Â checkpoint_interval):
â€Â Â Â Â 
â€Â Â Â Â initial_loop_stateÂ =Â (initial_state_tuple,Â dt,Â params)
â€Â Â Â Â 
â€Â Â Â Â defÂ single_step_pytree(loop_state):
â€Â Â Â Â Â Â Â Â state,Â dt_val,Â params_valÂ =Â loop_state
â€Â Â Â Â Â Â Â Â new_stateÂ =Â ccz4_step(state,Â dt_val,Â dx,Â params_val)
â€Â Â Â Â Â Â Â Â returnÂ (new_state,Â dt_val,Â params_val)
â€
â€Â Â Â Â defÂ checkpointed_step_pytree(loop_state):
â€Â Â Â Â Â Â Â Â state,Â dt_val,Â params_valÂ =Â loop_state
â€Â Â Â Â Â Â Â Â new_stateÂ =Â checkpointed_step(state,Â dt_val,Â dx,Â params_val)
â€Â Â Â Â Â Â Â Â returnÂ (new_state,Â dt_val,Â params_val)
â€Â Â Â Â 
â€Â Â Â Â defÂ body(i,Â loop_state):
â€Â Â Â Â Â Â Â Â returnÂ cond(iÂ %Â checkpoint_intervalÂ ==Â 0,
â€Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â checkpointed_step_pytree,
â€Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â single_step_pytree,Â Â Â Â Â Â Â 
â€Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â loop_state)
â€Â Â Â Â 
â€Â Â Â Â final_loop_stateÂ =Â fori_loop(0,Â num_steps,Â body,Â initial_loop_state)
â€Â Â Â Â final_state_tuple,Â _,Â _Â =Â final_loop_state
â€Â Â Â Â returnÂ final_state_tuple
â€
â€#Â CurryingÂ danÂ JITÂ untukÂ mengisolasiÂ argumenÂ statisÂ loop
â€full_adjoint_evolutionÂ =Â partial(jit,Â static_argnames=('num_steps',Â 'dx',Â 'checkpoint_interval'))(raw_full_adjoint_evolution)
â€
â€#Â =============================================================================
â€#Â ---Â 6.Â LOSSÂ FUNCTIONÂ DANÂ GRADIENTÂ ---
â€#Â =============================================================================
â€defÂ raw_loss_fn(initial_state_tuple,Â params,Â TEST_TIMESTEPS,Â GLOBAL_DT,Â dx,Â CHECKPOINT_INTERVAL):
â€Â Â Â Â """Loss:Â meminimalkanÂ pelanggaranÂ ConstraintÂ danÂ KÂ +Â MomentumÂ Constraint."""
â€Â Â Â Â 
â€Â Â Â Â finalÂ =Â full_adjoint_evolution(initial_state_tuple,Â TEST_TIMESTEPS,Â GLOBAL_DT,
â€Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â dx,Â params,Â CHECKPOINT_INTERVAL)
â€Â Â Â Â 
â€Â Â Â Â stateÂ =Â CCZ4State.from_tuple(final)
â€Â Â Â Â Mi_sqÂ =Â jnp.mean(momentum_constraint(state)**2)
â€Â Â Â Â 
â€Â Â Â Â #Â MenggunakanÂ bobotÂ K^2Â =Â 1.0Â dariÂ langkahÂ sebelumnya
â€Â Â Â Â returnÂ jnp.mean(state.Theta**2)Â +Â 1.0Â *Â jnp.mean(state.K**2)Â +Â Mi_sq
â€
â€#Â 1.Â LossÂ FunctionÂ UtamaÂ (di-curryÂ danÂ JIT)
â€jitted_loss_fnÂ =Â jit(partial(raw_loss_fn,Â TEST_TIMESTEPS=TEST_TIMESTEPS,Â GLOBAL_DT=GLOBAL_DT,
â€Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â dx=dx,Â CHECKPOINT_INTERVAL=CHECKPOINT_INTERVAL))
â€
â€#Â 2.Â GradienÂ terhadapÂ PyTreeÂ paramsÂ (indeksÂ 1)
â€grad_params_rawÂ =Â grad(lambdaÂ state,Â p:Â raw_loss_fn(state,Â p,Â TEST_TIMESTEPS,Â GLOBAL_DT,Â dx,Â CHECKPOINT_INTERVAL),Â argnums=1)
â€
â€#Â 3.Â WrapperÂ untukÂ mendapatkanÂ gradienÂ eta
â€defÂ get_eta_gradient(initial_state_tuple,Â params):
â€Â Â Â Â grad_ptreeÂ =Â grad_params_raw(initial_state_tuple,Â params)
â€Â Â Â Â returnÂ grad_ptree.eta
â€
â€#Â =============================================================================
â€#Â ---Â 7.Â GRADIENTÂ DESCENTÂ BENCHMARKÂ ---
â€#Â =============================================================================
â€
â€@jit
â€defÂ optimization_step(current_params,Â current_state_tuple):
â€Â Â Â Â 
â€Â Â Â Â grad_eta_rawÂ =Â get_eta_gradient(current_state_tuple,Â current_params)
â€Â Â Â Â grad_etaÂ =Â jnp.float32(grad_eta_raw)
â€Â Â Â Â 
â€Â Â Â Â loss_valÂ =Â jitted_loss_fn(current_state_tuple,Â current_params)
â€Â Â Â Â returnÂ loss_val,Â grad_etaÂ 
â€
â€defÂ run_gradient_descent_benchmark():
â€Â Â Â Â globalÂ N,Â dx,Â FIELD_SHAPE,Â CHECKPOINT_INTERVAL,Â TEST_TIMESTEPS,Â LEARNING_RATE,Â N_ITERATIONS
â€Â Â Â Â N_startÂ =Â N
â€Â Â Â Â 
â€Â Â Â Â FIELD_SHAPEÂ =Â (N,Â N,Â N)
â€
â€Â Â Â Â initial_stateÂ =Â initialize_minkowski(N)
â€Â Â Â Â state_tupleÂ =Â initial_state.to_tuple()
â€Â Â Â Â 
â€Â Â Â Â params_optÂ =Â PARAMSÂ 
â€Â Â Â Â opt_stateÂ =Â adam_init(params_opt)
â€Â Â Â Â 
â€Â Â Â Â print(f"---Â ğŸ†Â DIFF-CCZ4Â GRADIENTÂ DESCENTÂ TESTÂ N={N_start}^3Â /Â T={TEST_TIMESTEPS}Â (K1:Â {PARAMS.kappa1.item():.1f},Â K2:Â {PARAMS.kappa2.item():.1f},Â ETA_INIT:Â {PARAMS.eta.item():.1f},Â LR:Â {LEARNING_RATE:.0e},Â ITER:Â {N_ITERATIONS})Â ---")
â€Â Â Â Â 
â€Â Â Â Â #Â ---Â KompilasiÂ AwalÂ ---
â€Â Â Â Â print("CompilingÂ lossÂ function...",Â end="")
â€Â Â Â Â try:
â€Â Â Â Â Â Â Â Â loss_val_dummy,Â grad_eta_dummyÂ =Â optimization_step(params_opt,Â state_tuple)
â€Â Â Â Â Â Â Â Â loss_val_dummy.block_until_ready()
â€Â Â Â Â Â Â Â Â print("DONE")
â€Â Â Â Â exceptÂ ExceptionÂ asÂ e:
â€Â Â Â Â Â Â Â Â print(f"ERROR:Â KompilasiÂ Gagal:Â {e}")
â€Â Â Â Â Â Â Â Â return
â€
â€Â Â Â Â loss_historyÂ =Â []
â€Â Â Â Â 
â€Â Â Â Â #Â MAINÂ OPTIMIZATIONÂ LOOP
â€Â Â Â Â forÂ stepÂ inÂ range(1,Â N_ITERATIONSÂ +Â 1):
â€Â Â Â Â Â Â Â Â start_stepÂ =Â time.time()
â€Â Â Â Â Â Â Â Â 
â€Â Â Â Â Â Â Â Â loss_val,Â grad_etaÂ =Â optimization_step(params_opt,Â state_tuple)
â€Â Â Â Â Â Â Â Â 
â€Â Â Â Â Â Â Â Â #Â MenggunakanÂ LEARNING_RATEÂ =Â 1.0
â€Â Â Â Â Â Â Â Â opt_state,Â delta_etaÂ =Â adam_update(step,Â grad_eta,Â opt_state,Â learning_rate=LEARNING_RATE)
â€Â Â Â Â Â Â Â Â 
â€Â Â Â Â Â Â Â Â new_etaÂ =Â params_opt.etaÂ -Â delta_eta
â€Â Â Â Â Â Â Â Â params_optÂ =Â params_opt.replace(eta=new_eta)
â€Â Â Â Â Â Â Â Â 
â€Â Â Â Â Â Â Â Â loss_val.block_until_ready()
â€Â Â Â Â Â Â Â Â step_timeÂ =Â time.time()Â -Â start_step
â€Â Â Â Â Â Â Â Â loss_history.append(loss_val)
â€Â Â Â Â Â Â Â Â 
â€Â Â Â Â Â Â Â Â ifÂ stepÂ %Â 50Â ==Â 0Â orÂ stepÂ <=Â 10:
â€Â Â Â Â Â Â Â Â Â Â Â Â print(f"StepÂ {step:03d}Â |Â Loss:Â {loss_val.item():.2e}Â |Â GradÂ E:Â {grad_eta.item():.2e}Â |Â Time:Â {step_time:.2f}sÂ |Â E:Â {params_opt.eta.item():.4f}")
â€Â Â Â Â Â Â Â Â 
â€Â Â Â Â print("\n---Â âœ…Â HASILÂ AKHIRÂ OPTIMASIÂ ---")
â€Â Â Â Â print(f"LossÂ AwalÂ (StepÂ 1):Â {loss_history[0].item():.2e}")
â€Â Â Â Â print(f"LossÂ AkhirÂ (StepÂ {N_ITERATIONS}):Â {loss_history[-1].item():.2e}")
â€Â Â Â Â print(f"NilaiÂ AkhirÂ Eta:Â {params_opt.eta.item():.4f}")
â€Â Â Â Â 
â€Â Â Â Â ifÂ loss_history[-1]Â <Â loss_history[0]Â *Â 0.5:
â€Â Â Â Â Â Â Â Â print("STATUS:Â SUCCESS.Â LossÂ turunÂ signifikan.Â EÂ bergerakÂ keÂ nilaiÂ optimal.")
â€Â Â Â Â elifÂ loss_history[-1]Â <Â loss_history[0]:
â€Â Â Â Â Â Â Â Â print("STATUS:Â SUCCESS.Â LossÂ turun.Â EÂ bergerakÂ keÂ nilaiÂ yangÂ benar.")
â€Â Â Â Â else:
â€Â Â Â Â Â Â Â Â print("STATUS:Â CAUTION.Â PeriksaÂ apakahÂ LossÂ atauÂ EÂ bergerakÂ keÂ arahÂ yangÂ salah.")
â€
â€ifÂ __name__Â ==Â "__main__":
â€Â Â Â Â run_gradient_descent_benchmark()
â€
