â€
â€#Â FILE:Â diff_ccz4_gradient_descent_test_v14_Koreksi_Final.py
â€#Â TESTÂ 11:Â OPTIMASIÂ KAPPA2Â (K1Â DIKUNCIÂ PADAÂ 2.0)Â -Â KOREKSIÂ FINAL
â€
â€importÂ jax
â€importÂ jax.numpyÂ asÂ jnp
â€fromÂ jaxÂ importÂ jit,Â grad,Â checkpoint
â€fromÂ jax.laxÂ importÂ fori_loop,Â cond
â€fromÂ jax.tree_utilÂ importÂ register_pytree_node_class,Â tree_mapÂ 
â€fromÂ functoolsÂ importÂ partial
â€importÂ time
â€
â€#Â =============================================================================
â€#Â ---Â OPTIMIZERÂ ADAMÂ DARIÂ JAX/OPTICSÂ ---
â€#Â =============================================================================
â€@register_pytree_node_class
â€classÂ AdamState:
â€Â Â Â Â defÂ __init__(self,Â count,Â m_kappa2,Â v_kappa2):
â€Â Â Â Â Â Â Â Â self.countÂ =Â count
â€Â Â Â Â Â Â Â Â self.m_kappa2Â =Â m_kappa2
â€Â Â Â Â Â Â Â Â self.v_kappa2Â =Â v_kappa2
â€Â Â Â Â Â Â Â Â 
â€Â Â Â Â defÂ replace(self,Â **kwargs):
â€Â Â Â Â Â Â Â Â returnÂ AdamState(
â€Â Â Â Â Â Â Â Â Â Â Â Â kwargs.get('count',Â self.count),
â€Â Â Â Â Â Â Â Â Â Â Â Â kwargs.get('m_kappa2',Â self.m_kappa2),
â€Â Â Â Â Â Â Â Â Â Â Â Â kwargs.get('v_kappa2',Â self.v_kappa2)
â€Â Â Â Â Â Â Â Â )
â€
â€Â Â Â Â defÂ tree_flatten(self):
â€Â Â Â Â Â Â Â Â childrenÂ =Â (self.count,Â self.m_kappa2,Â self.v_kappa2)
â€Â Â Â Â Â Â Â Â returnÂ children,Â None
â€Â Â Â Â 
â€Â Â Â Â @classmethod
â€Â Â Â Â defÂ tree_unflatten(cls,Â aux_data,Â children):
â€Â Â Â Â Â Â Â Â returnÂ cls(*children)
â€
â€defÂ adam_init(params):
â€Â Â Â Â returnÂ AdamState(jnp.array(0),Â jnp.zeros_like(params.kappa2),Â jnp.zeros_like(params.kappa2))
â€
â€defÂ adam_update(i,Â grad_kappa2,Â state,Â learning_rate=1e-3,Â b1=0.9,Â b2=0.999,Â eps=1e-8):
â€Â Â Â Â stateÂ =Â state.replace(count=state.countÂ +Â 1)
â€Â Â Â Â iÂ =Â state.count
â€Â Â Â Â 
â€Â Â Â Â mÂ =Â (b1Â *Â state.m_kappa2)Â +Â (1Â -Â b1)Â *Â grad_kappa2
â€Â Â Â Â vÂ =Â (b2Â *Â state.v_kappa2)Â +Â (1Â -Â b2)Â *Â (grad_kappa2Â *Â grad_kappa2)
â€Â Â Â Â 
â€Â Â Â Â m_hatÂ =Â mÂ /Â (1Â -Â b1**i)
â€Â Â Â Â v_hatÂ =Â vÂ /Â (1Â -Â b2**i)
â€Â Â Â Â 
â€Â Â Â Â delta_kappa2Â =Â learning_rateÂ *Â m_hatÂ /Â (jnp.sqrt(v_hat)Â +Â eps)
â€Â Â Â Â 
â€Â Â Â Â new_stateÂ =Â AdamState(state.count,Â m,Â v)
â€Â Â Â Â returnÂ new_state,Â delta_kappa2Â 
â€
â€#Â =============================================================================
â€#Â ---Â 0.Â KONFIGURASIÂ DANÂ PARAMETERÂ ---
â€#Â =============================================================================
â€NÂ =Â 10Â Â Â Â Â Â Â Â Â Â 
â€dxÂ =Â 0.1
â€FIELD_SHAPEÂ =Â (N,Â N,Â N)
â€DIMÂ =Â 3
â€
â€@register_pytree_node_class
â€classÂ Params:
â€Â Â Â Â defÂ __init__(self,Â kappa1,Â kappa2,Â eta):
â€Â Â Â Â Â Â Â Â self.kappa1Â =Â kappa1
â€Â Â Â Â Â Â Â Â self.kappa2Â =Â kappa2
â€Â Â Â Â Â Â Â Â self.etaÂ =Â eta
â€Â Â Â Â Â Â Â Â 
â€Â Â Â Â defÂ replace(self,Â **kwargs):
â€Â Â Â Â Â Â Â Â returnÂ Params(
â€Â Â Â Â Â Â Â Â Â Â Â Â kwargs.get('kappa1',Â self.kappa1),
â€Â Â Â Â Â Â Â Â Â Â Â Â kwargs.get('kappa2',Â self.kappa2),
â€Â Â Â Â Â Â Â Â Â Â Â Â kwargs.get('eta',Â self.eta)
â€Â Â Â Â Â Â Â Â )
â€
â€Â Â Â Â defÂ tree_flatten(self):
â€Â Â Â Â Â Â Â Â returnÂ (self.kappa1,Â self.kappa2,Â self.eta),Â None
â€Â Â Â Â 
â€Â Â Â Â @classmethod
â€Â Â Â Â defÂ tree_unflatten(cls,Â aux_data,Â children):
â€Â Â Â Â Â Â Â Â returnÂ cls(*children)
â€
â€GLOBAL_DTÂ =Â 0.005
â€
â€#Â KAPPA1Â DIKUNCIÂ padaÂ 2.0.Â KAPPA2Â dimulaiÂ dariÂ 1.0
â€PARAMSÂ =Â Params(kappa1=2.0,Â kappa2=1.0,Â eta=2.0)
â€
â€TEST_TIMESTEPSÂ =Â 200Â Â Â Â Â Â 
â€CHECKPOINT_INTERVALÂ =Â 5Â Â Â 
â€LEARNING_RATEÂ =Â 5e-2Â Â Â Â Â Â 
â€N_ITERATIONSÂ =Â 200Â Â Â Â Â Â Â Â 
â€
â€#Â =============================================================================
â€#Â ---Â 1.Â CCZ4Â STATEÂ (PYTREE)Â &Â INITIALIZATIONÂ ---
â€#Â =============================================================================
â€@register_pytree_node_class
â€classÂ CCZ4State:
â€Â Â Â Â defÂ __init__(self,Â phi,Â chi,Â K,Â At,Â Gam,Â Theta,Â Z,Â alpha,Â beta):
â€Â Â Â Â Â Â Â Â self.phi,Â self.chi,Â self.K,Â self.At,Â self.Gam,Â self.Theta,Â self.Z,Â self.alpha,Â self.betaÂ =Â \
â€Â Â Â Â Â Â Â Â Â Â Â Â phi,Â chi,Â K,Â At,Â Gam,Â Theta,Â Z,Â alpha,Â beta
â€Â Â Â Â defÂ to_tuple(self):
â€Â Â Â Â Â Â Â Â returnÂ (self.phi,Â self.chi,Â self.K,Â self.At,Â self.Gam,Â self.Theta,Â self.Z,Â self.alpha,Â self.beta)
â€Â Â Â Â @classmethod
â€Â Â Â Â defÂ from_tuple(cls,Â t):
â€Â Â Â Â Â Â Â Â returnÂ cls(*t)
â€Â Â Â Â defÂ tree_flatten(self):
â€Â Â Â Â Â Â Â Â returnÂ self.to_tuple(),Â None
â€Â Â Â Â @classmethod
â€Â Â Â Â defÂ tree_unflatten(cls,Â aux_data,Â children):
â€Â Â Â Â Â Â Â Â returnÂ cls(*children)
â€
â€defÂ initialize_minkowski(N):
â€Â Â Â Â zerosÂ =Â jnp.zeros(FIELD_SHAPE)
â€Â Â Â Â onesÂ =Â jnp.ones(FIELD_SHAPE)
â€Â Â Â Â rÂ =Â jnp.linalg.norm(jnp.mgrid[:N,:N,:N]Â -Â N/2,Â axis=0)Â *Â dx
â€Â Â Â Â alpha_initÂ =Â onesÂ -Â 0.01Â *Â jnp.exp(-r**2Â /Â 5.0)
â€Â Â Â Â perturbationÂ =Â 1e-2Â *Â jnp.exp(-r**2Â /Â 5.0)
â€Â Â Â Â K_initÂ =Â zerosÂ +Â perturbation
â€Â Â Â Â returnÂ CCZ4State(zeros,Â ones,Â K_init,Â jnp.zeros((5,)Â +Â FIELD_SHAPE),
â€Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â jnp.zeros((3,)Â +Â FIELD_SHAPE),Â zeros,Â jnp.zeros((3,)Â +Â FIELD_SHAPE),
â€Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â alpha_init,Â jnp.zeros((3,)Â +Â FIELD_SHAPE))
â€
â€#Â =============================================================================
â€#Â ---Â 2.Â ALGEBRAÂ TENSORÂ RIILÂ &Â CONSTRAINTSÂ ---
â€#Â =============================================================================
â€defÂ diff6(f,Â dx):
â€Â Â Â Â d_xÂ =Â jnp.roll(f,Â -1,Â axis=0)Â -Â jnp.roll(f,Â 1,Â axis=0)
â€Â Â Â Â d_yÂ =Â jnp.roll(f,Â -1,Â axis=1)Â -Â jnp.roll(f,Â 1,Â axis=1)
â€Â Â Â Â d_zÂ =Â jnp.roll(f,Â -1,Â axis=2)Â -Â jnp.roll(f,Â 1,Â axis=2)
â€Â Â Â Â returnÂ jnp.stack([d_x,Â d_y,Â d_z],Â axis=0)Â /Â (2.0Â *Â dx)
â€defÂ grad6(f):Â returnÂ diff6(f,Â dx)
â€defÂ advect(f,Â beta,Â d):
â€Â Â Â Â grad_fÂ =Â d(f)
â€Â Â Â Â returnÂ beta[0]*grad_f[0]Â +Â beta[1]*grad_f[1]Â +Â beta[2]*grad_f[2]
â€
â€@jit
â€defÂ voigt_to_tensor(A_voigt):
â€Â Â Â Â shapeÂ =Â A_voigt.shape[1:]
â€Â Â Â Â A_fullÂ =Â jnp.zeros((DIM,Â DIM)Â +Â shape)
â€Â Â Â Â A_fullÂ =Â A_full.at[0,0].set(A_voigt[0])
â€Â Â Â Â A_fullÂ =Â A_full.at[1,1].set(A_voigt[3])
â€Â Â Â Â A_fullÂ =Â A_full.at[2,2].set(-A_voigt[0]Â -Â A_voigt[3])
â€Â Â Â Â A_fullÂ =Â A_full.at[0,1].set(A_voigt[1]);Â A_fullÂ =Â A_full.at[1,0].set(A_voigt[1])
â€Â Â Â Â A_fullÂ =Â A_full.at[0,2].set(A_voigt[2]);Â A_fullÂ =Â A_full.at[2,0].set(A_voigt[2])
â€Â Â Â Â A_fullÂ =Â A_full.at[1,2].set(A_voigt[4]);Â A_fullÂ =Â A_full.at[2,1].set(A_voigt[4])
â€Â Â Â Â returnÂ A_full
â€
â€@jit
â€defÂ get_gamma_inv(state):
â€Â Â Â Â returnÂ jnp.identity(DIM)[:,Â :,Â None,Â None,Â None]Â *Â jnp.ones((DIM,Â DIM)Â +Â FIELD_SHAPE)
â€
â€@jit
â€defÂ hamiltonian_constraint(state):
â€Â Â Â Â chiÂ =Â state.chi
â€Â Â Â Â At_fullÂ =Â voigt_to_tensor(state.At)
â€Â Â Â Â gamma_invÂ =Â get_gamma_inv(state)
â€Â Â Â Â At_sqÂ =Â jnp.einsum('ij...,Â kl...,Â ik...,Â jl...Â ->Â ...',
â€Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â At_full,Â At_full,Â gamma_inv,Â gamma_inv)
â€Â Â Â Â log_chiÂ =Â jnp.log(chiÂ +Â 1e-15)
â€Â Â Â Â d_log_chiÂ =Â grad6(log_chi)
â€Â Â Â Â grad_sq_sumÂ =Â jnp.sum(d_log_chiÂ *Â d_log_chi,Â axis=0)
â€Â Â Â Â R_chiÂ =Â -8Â *Â grad_sq_sumÂ 
â€Â Â Â Â HÂ =Â (R_chiÂ +Â state.K**2Â -Â At_sq)Â /Â chiÂ +Â 2.0Â *Â state.KÂ *Â state.ThetaÂ 
â€Â Â Â Â returnÂ H
â€
â€@jit
â€defÂ momentum_constraint(state):
â€Â Â Â Â At_fullÂ =Â voigt_to_tensor(state.At)
â€Â Â Â Â d_AtÂ =Â grad6(At_full)
â€Â Â Â Â MiÂ =Â jnp.einsum('jk...,Â kij...Â ->Â i...',
â€Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â get_gamma_inv(state),Â d_At)
â€Â Â Â Â returnÂ MiÂ *Â 0.1
â€
â€#Â =============================================================================
â€#Â ---Â 3.Â CCZ4Â RHSÂ (MASTERÂ EVOLUTIONÂ EQUATIONS)Â ---
â€#Â =============================================================================
â€defÂ ccz4_rhs(state_tuple,Â dx,Â params):
â€Â Â Â Â stateÂ =Â CCZ4State.from_tuple(state_tuple)
â€Â Â Â Â dÂ =Â lambdaÂ f:Â diff6(f,Â dx)
â€Â Â Â Â LieÂ =Â lambdaÂ f:Â advect(f,Â state.beta,Â d)
â€Â Â Â Â HÂ =Â hamiltonian_constraint(state)
â€Â Â Â Â MiÂ =Â momentum_constraint(state)
â€Â Â Â Â 
â€Â Â Â Â alphaÂ =Â state.alpha
â€Â Â Â Â KÂ =Â state.K
â€Â Â Â Â At_fullÂ =Â voigt_to_tensor(state.At)
â€Â Â Â Â 
â€Â Â Â Â dTheta_dtÂ =Â (
â€Â Â Â Â Â Â Â Â Lie(state.Theta)
â€Â Â Â Â Â Â Â Â +Â 0.5Â *Â alphaÂ *Â H
â€Â Â Â Â Â Â Â Â -Â alphaÂ *Â (2.0Â -Â params.kappa1)Â *Â KÂ *Â state.Theta
â€Â Â Â Â Â Â Â Â -Â params.kappa1Â *Â alphaÂ *Â state.Theta
â€Â Â Â Â )
â€Â Â Â Â dZ_dtÂ =Â (
â€Â Â Â Â Â Â Â Â Lie(state.Z)
â€Â Â Â Â Â Â Â Â +Â alphaÂ *Â (MiÂ -Â (2/3)Â *Â d(K)Â *Â state.Theta)
â€Â Â Â Â Â Â Â Â -Â params.kappa2Â *Â alphaÂ *Â state.ZÂ 
â€Â Â Â Â )
â€
â€Â Â Â Â dphi_dtÂ =Â jnp.zeros_like(state.phi)
â€Â Â Â Â dchi_dtÂ =Â Lie(state.chi)Â +Â (1.0Â /Â 3.0)Â *Â state.chiÂ *Â alphaÂ *Â K
â€Â Â Â Â dK_dtÂ =Â (
â€Â Â Â Â Â Â Â Â Lie(K)
â€Â Â Â Â Â Â Â Â +Â alphaÂ *Â (K**2Â /Â 3.0Â +Â HÂ /Â 2.0)
â€Â Â Â Â Â Â Â Â +Â alphaÂ *Â 0.0
â€Â Â Â Â )
â€
â€Â Â Â Â Ricc_proxyÂ =Â KÂ *Â KÂ *Â jnp.identity(DIM)[:,Â :,Â None,Â None,Â None]Â *Â 0.1
â€Â Â Â Â dAt_dtÂ =Â (
â€Â Â Â Â Â Â Â Â Lie(At_full)
â€Â Â Â Â Â Â Â Â -Â alphaÂ *Â (Ricc_proxyÂ -Â KÂ *Â At_full)
â€Â Â Â Â Â Â Â Â -Â 2.0Â *Â alphaÂ *Â KÂ *Â At_fullÂ 
â€Â Â Â Â )
â€Â Â Â Â dAt_dt_voigtÂ =Â jnp.stack([dAt_dt[0,0],Â dAt_dt[0,1],Â dAt_dt[0,2],Â dAt_dt[1,1],Â dAt_dt[1,2]],Â axis=0)
â€
â€Â Â Â Â dGam_dtÂ =Â Lie(state.Gam)Â -Â params.etaÂ *Â alphaÂ *Â state.GamÂ +Â alphaÂ *Â Mi
â€Â Â Â Â dalpha_dtÂ =Â -2.0Â *Â alphaÂ *Â K
â€Â Â Â Â dbeta_dtÂ =Â 0.75Â *Â state.GamÂ -Â params.etaÂ *Â state.beta
â€
â€Â Â Â Â returnÂ (dphi_dt,Â dchi_dt,Â dK_dt,Â dAt_dt_voigt,Â dGam_dt,Â dTheta_dt,Â dZ_dt,Â dalpha_dt,Â dbeta_dt)
â€
â€
â€#Â =============================================================================
â€#Â ---Â 4.Â INTEGRATORÂ &Â STEPÂ FUNCTIONSÂ ---
â€#Â =============================================================================
â€defÂ rk4_step(state_tuple,Â rhs_fn,Â dt,Â dx,Â params):
â€Â Â Â Â 
â€Â Â Â Â defÂ add_scaled(state,Â k,Â scale):
â€Â Â Â Â Â Â Â Â returnÂ tree_map(lambdaÂ x,Â y:Â xÂ +Â scaleÂ *Â y,Â state,Â k)
â€
â€Â Â Â Â k1Â =Â rhs_fn(state_tuple,Â dx,Â params)
â€Â Â Â Â k2Â =Â rhs_fn(add_scaled(state_tuple,Â k1,Â 0.5Â *Â dt),Â dx,Â params)
â€Â Â Â Â k3Â =Â rhs_fn(add_scaled(state_tuple,Â k2,Â 0.5Â *Â dt),Â dx,Â params)
â€Â Â Â Â k4Â =Â rhs_fn(add_scaled(state_tuple,Â k3,Â dt),Â dx,Â params)
â€Â Â Â Â 
â€Â Â Â Â final_stepÂ =Â tree_map(lambdaÂ k1_c,Â k2_c,Â k3_c,Â k4_c:Â (dtÂ /Â 6.0)Â *Â (k1_cÂ +Â 2*k2_cÂ +Â 2*k3_cÂ +Â k4_c),
â€Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â k1,Â k2,Â k3,Â k4)
â€Â Â Â Â 
â€Â Â Â Â returnÂ tree_map(lambdaÂ s,Â f:Â sÂ +Â f,Â state_tuple,Â final_step)
â€
â€defÂ ccz4_step(state_tuple,Â dt,Â dx,Â params):
â€Â Â Â Â returnÂ rk4_step(state_tuple,Â ccz4_rhs,Â dt,Â dx,Â params)
â€
â€checkpointed_stepÂ =Â checkpoint(ccz4_step)
â€
â€#Â =============================================================================
â€#Â ---Â 5.Â LOOPINGÂ (EVOLUTION)Â ---
â€#Â =============================================================================
â€@partial(jit,Â static_argnames=('num_steps',Â 'dx',Â 'checkpoint_interval'))
â€defÂ full_adjoint_evolution(initial_state_tuple,Â num_steps,Â dt,Â dx,Â params,Â checkpoint_interval):
â€Â Â Â Â 
â€Â Â Â Â initial_loop_stateÂ =Â (initial_state_tuple,Â dt,Â params)
â€Â Â Â Â 
â€Â Â Â Â defÂ single_step_pytree(loop_state):
â€Â Â Â Â Â Â Â Â state,Â dt_val,Â params_valÂ =Â loop_state
â€Â Â Â Â Â Â Â Â new_stateÂ =Â ccz4_step(state,Â dt_val,Â dx,Â params_val)
â€Â Â Â Â Â Â Â Â returnÂ (new_state,Â dt_val,Â params_val)
â€
â€Â Â Â Â defÂ checkpointed_step_pytree(loop_state):
â€Â Â Â Â Â Â Â Â state,Â dt_val,Â params_valÂ =Â loop_state
â€Â Â Â Â Â Â Â Â new_stateÂ =Â checkpointed_step(state,Â dt_val,Â dx,Â params_val)
â€Â Â Â Â Â Â Â Â returnÂ (new_state,Â dt_val,Â params_val)
â€Â Â Â Â 
â€Â Â Â Â defÂ body(i,Â loop_state):
â€Â Â Â Â Â Â Â Â returnÂ cond(iÂ %Â checkpoint_intervalÂ ==Â 0,
â€Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â checkpointed_step_pytree,
â€Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â single_step_pytree,Â Â Â Â Â Â Â 
â€Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â loop_state)
â€Â Â Â Â 
â€Â Â Â Â final_loop_stateÂ =Â fori_loop(0,Â num_steps,Â body,Â initial_loop_state)
â€Â Â Â Â final_state_tuple,Â _,Â _Â =Â final_loop_state
â€Â Â Â Â returnÂ final_state_tuple
â€
â€#Â =============================================================================
â€#Â ---Â 6.Â LOSSÂ FUNCTIONÂ DANÂ GRADIENTÂ ---
â€#Â =============================================================================
â€@partial(jit,Â static_argnames=('TEST_TIMESTEPS',Â 'GLOBAL_DT',Â 'dx',Â 'CHECKPOINT_INTERVAL'))
â€defÂ loss_fn(initial_state_tuple,Â params,Â TEST_TIMESTEPS,Â GLOBAL_DT,Â dx,Â CHECKPOINT_INTERVAL):
â€Â Â Â Â """Loss:Â meminimalkanÂ pelanggaranÂ ConstraintÂ danÂ KÂ +Â MomentumÂ Constraint."""
â€Â Â Â Â finalÂ =Â full_adjoint_evolution(initial_state_tuple,Â TEST_TIMESTEPS,Â GLOBAL_DT,
â€Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â dx,Â params,Â CHECKPOINT_INTERVAL)
â€Â Â Â Â 
â€Â Â Â Â stateÂ =Â CCZ4State.from_tuple(final)
â€Â Â Â Â #Â Loss:Â Theta^2Â (Hamiltonian)Â +Â K^2Â +Â Mi^2Â (Momentum)
â€Â Â Â Â Mi_sqÂ =Â jnp.mean(momentum_constraint(state)**2)
â€Â Â Â Â returnÂ jnp.mean(state.Theta**2)Â +Â 1e-8Â *Â jnp.mean(state.K**2)Â +Â Mi_sq
â€
â€#Â GradienÂ terhadapÂ parameterÂ (indeksÂ 1)Â saja.
â€grad_lossÂ =Â grad(loss_fn,Â argnums=(1))
â€
â€#Â =============================================================================
â€#Â ---Â 7.Â GRADIENTÂ DESCENTÂ BENCHMARKÂ ---
â€#Â =============================================================================
â€
â€defÂ run_gradient_descent_benchmark():
â€Â Â Â Â globalÂ N,Â dx,Â FIELD_SHAPE,Â CHECKPOINT_INTERVAL,Â TEST_TIMESTEPS,Â LEARNING_RATE,Â N_ITERATIONS
â€Â Â Â Â N_startÂ =Â N
â€Â Â Â Â 
â€Â Â Â Â FIELD_SHAPEÂ =Â (N,Â N,Â N)
â€
â€Â Â Â Â initial_stateÂ =Â initialize_minkowski(N)
â€Â Â Â Â state_tupleÂ =Â initial_state.to_tuple()
â€Â Â Â Â 
â€Â Â Â Â params_optÂ =Â PARAMSÂ 
â€Â Â Â Â opt_stateÂ =Â adam_init(params_opt)
â€Â Â Â Â 
â€Â Â Â Â print(f"---Â ğŸ†Â DIFF-CCZ4Â GRADIENTÂ DESCENTÂ TESTÂ N={N_start}^3Â /Â T={TEST_TIMESTEPS}Â (K1:Â {PARAMS.kappa1:.1f},Â K2_INIT:Â {PARAMS.kappa2:.1f},Â LR:Â {LEARNING_RATE:.0e},Â ITER:Â {N_ITERATIONS})Â ---")
â€Â Â Â Â 
â€Â Â Â Â #Â ---Â KompilasiÂ AwalÂ ---
â€Â Â Â Â print("CompilingÂ lossÂ function...",Â end="")
â€Â Â Â Â try:
â€Â Â Â Â Â Â Â Â _Â =Â loss_fn(state_tuple,Â params_opt,Â TEST_TIMESTEPS,Â GLOBAL_DT,Â dx,Â CHECKPOINT_INTERVAL).block_until_ready()
â€Â Â Â Â Â Â Â Â print("DONE")
â€Â Â Â Â exceptÂ ExceptionÂ asÂ e:
â€Â Â Â Â Â Â Â Â print(f"ERROR:Â KompilasiÂ Gagal:Â {e}")
â€Â Â Â Â Â Â Â Â return
â€
â€Â Â Â Â loss_historyÂ =Â []
â€Â Â Â Â 
â€Â Â Â Â @jit
â€Â Â Â Â defÂ optimization_step(current_params,Â current_state_tuple):
â€Â Â Â Â Â Â Â Â #Â GradienÂ hanyaÂ untukÂ parameterÂ (indeksÂ 0Â dariÂ outputÂ grad_loss)
â€Â Â Â Â Â Â Â Â #Â KOREKSIÂ FINAL:Â HapusÂ unpacking.Â AmbilÂ objekÂ ParamsÂ gradienÂ langsung
â€Â Â Â Â Â Â Â Â grad_params_treeÂ =Â grad_loss(current_state_tuple,Â current_params,Â TEST_TIMESTEPS,Â GLOBAL_DT,Â dx,Â CHECKPOINT_INTERVAL)
â€Â Â Â Â Â Â Â Â 
â€Â Â Â Â Â Â Â Â #Â AmbilÂ skalarÂ KAPPA2Â dariÂ objekÂ gradien
â€Â Â Â Â Â Â Â Â grad_kappa2Â =Â grad_params_tree.kappa2
â€Â Â Â Â Â Â Â Â 
â€Â Â Â Â Â Â Â Â loss_valÂ =Â loss_fn(current_state_tuple,Â current_params,Â TEST_TIMESTEPS,Â GLOBAL_DT,Â dx,Â CHECKPOINT_INTERVAL)
â€Â Â Â Â Â Â Â Â returnÂ loss_val,Â grad_kappa2Â #Â MengembalikanÂ skalar
â€
â€Â Â Â Â #Â MAINÂ OPTIMIZATIONÂ LOOP
â€Â Â Â Â forÂ stepÂ inÂ range(1,Â N_ITERATIONSÂ +Â 1):
â€Â Â Â Â Â Â Â Â start_stepÂ =Â time.time()
â€Â Â Â Â Â Â Â Â 
â€Â Â Â Â Â Â Â Â #Â 1.Â RunÂ JITtedÂ functionÂ untukÂ mendapatkanÂ LossÂ danÂ GradienÂ KAPPA2
â€Â Â Â Â Â Â Â Â loss_val,Â grad_kappa2Â =Â optimization_step(params_opt,Â state_tuple)
â€Â Â Â Â Â Â Â Â 
â€Â Â Â Â Â Â Â Â #Â 2.Â UpdateÂ AdamÂ stateÂ danÂ dapatkanÂ perubahanÂ kappa2
â€Â Â Â Â Â Â Â Â opt_state,Â delta_kappa2Â =Â adam_update(step,Â grad_kappa2,Â opt_state,Â learning_rate=LEARNING_RATE)
â€Â Â Â Â Â Â Â Â 
â€Â Â Â Â Â Â Â Â #Â 3.Â TerapkanÂ pembaruanÂ padaÂ KAPPA2.Â KAPPA1Â tetapÂ 2.0
â€Â Â Â Â Â Â Â Â new_kappa2Â =Â params_opt.kappa2Â -Â delta_kappa2
â€Â Â Â Â Â Â Â Â params_optÂ =Â params_opt.replace(kappa2=new_kappa2)
â€Â Â Â Â Â Â Â Â 
â€Â Â Â Â Â Â Â Â #Â SinkronisasiÂ danÂ logging
â€Â Â Â Â Â Â Â Â loss_val.block_until_ready()
â€Â Â Â Â Â Â Â Â step_timeÂ =Â time.time()Â -Â start_step
â€Â Â Â Â Â Â Â Â loss_history.append(loss_val)
â€Â Â Â Â Â Â Â Â 
â€Â Â Â Â Â Â Â Â #Â CetakÂ outputÂ diÂ awalÂ (StepÂ 1-10)Â danÂ setiapÂ 50Â langkah
â€Â Â Â Â Â Â Â Â ifÂ stepÂ %Â 50Â ==Â 0Â orÂ stepÂ <=Â 10:
â€Â Â Â Â Â Â Â Â Â Â Â Â print(f"StepÂ {step:03d}Â |Â Loss:Â {loss_val:.2e}Â |Â GradÂ K2:Â {grad_kappa2:.2e}Â |Â Time:Â {step_time:.2f}sÂ |Â K2:Â {params_opt.kappa2:.4f}")
â€Â Â Â Â Â Â Â Â 
â€Â Â Â Â print("\n---Â âœ…Â HASILÂ AKHIRÂ OPTIMASIÂ ---")
â€Â Â Â Â print(f"LossÂ AwalÂ (StepÂ 1):Â {loss_history[0]:.2e}")
â€Â Â Â Â print(f"LossÂ AkhirÂ (StepÂ {N_ITERATIONS}):Â {loss_history[-1]:.2e}")
â€Â Â Â Â print(f"NilaiÂ AkhirÂ Kappa2:Â {params_opt.kappa2:.4f}")
â€Â Â Â Â 
â€Â Â Â Â ifÂ loss_history[-1]Â <Â loss_history[0]Â *Â 0.5:
â€Â Â Â Â Â Â Â Â print("STATUS:Â SUCCESS.Â LossÂ turunÂ signifikan.Â K2Â bergerakÂ keÂ nilaiÂ optimal.")
â€Â Â Â Â elifÂ loss_history[-1]Â <Â loss_history[0]:
â€Â Â Â Â Â Â Â Â print("STATUS:Â SUCCESS.Â LossÂ turun.Â K2Â bergerakÂ keÂ nilaiÂ yangÂ benar.")
â€Â Â Â Â else:
â€Â Â Â Â Â Â Â Â print("STATUS:Â CAUTION.Â PeriksaÂ apakahÂ LossÂ atauÂ K2Â bergerakÂ keÂ arahÂ yangÂ salah.")
â€
â€ifÂ __name__Â ==Â "__main__":
â€Â Â Â Â run_gradient_descent_benchmark()
â€
