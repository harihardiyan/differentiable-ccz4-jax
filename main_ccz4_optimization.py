# FILE: main_ccz4_optimization.py
# Machine Discovery of CCZ4 Universal Stability Parameters using JAX Autodiff.
# Optimization Target: kappa1 (The known optimal value is 2.0).

import jax
import jax.numpy as jnp
from jax import jit, grad, checkpoint
from jax.lax import fori_loop, cond
from jax.tree_util import register_pytree_node_class, tree_map
from functools import partial
import time

# jax.config.update("jax_enable_x64", True) # Optional: Uncomment for double precision
# =============================================================================
# --- 0. CONFIGURATION AND PARAMETERS ---
# =============================================================================
N = 10
dx = 0.1
FIELD_SHAPE = (N, N, N)
DIM = 3

@register_pytree_node_class
class Params:
    # Only kappa1 will be optimized; others remain constant
    def __init__(self, kappa1, kappa2, eta):
        self.kappa1 = jnp.asarray(kappa1, dtype=jnp.float32)
        self.kappa2 = jnp.asarray(kappa2, dtype=jnp.float32)
        self.eta = jnp.asarray(eta, dtype=jnp.float32)

    def replace(self, **kwargs):
        return Params(
            kwargs.get('kappa1', self.kappa1),
            kwargs.get('kappa2', self.kappa2),
            kwargs.get('eta', self.eta)
        )

    def tree_flatten(self):
        return (self.kappa1, self.kappa2, self.eta), None

    @classmethod
    def tree_unflatten(cls, aux_data, children):
        return cls(*children)

GLOBAL_DT = 0.005

# Set kappa1 far from 2.0 (optimal) to demonstrate convergence
PARAMS = Params(kappa1=0.0, kappa2=1.0, eta=2.0)

TEST_TIMESTEPS = 200
CHECKPOINT_INTERVAL = 5
# Aggressive Learning Rate (proven necessary)
LEARNING_RATE = 1.0
N_ITERATIONS = 200

# =============================================================================
# --- ADAM OPTIMIZER (JAX/OPTICS STYLE) ---
# =============================================================================
@register_pytree_node_class
class AdamState:
    # Tracking m and v for kappa1
    def __init__(self, count, m_k1, v_k1):
        self.count = count
        self.m_k1 = m_k1
        self.v_k1 = v_k1

    def replace(self, **kwargs):
            return AdamState(
                kwargs.get('count', self.count),
                kwargs.get('m_k1', self.m_k1),
                kwargs.get('v_k1', self.v_k1)
            )

    def tree_flatten(self):
        children = (self.count, self.m_k1, self.v_k1)
        return children, None

    @classmethod
    def tree_unflatten(cls, aux_data, children):
        return cls(*children)

def adam_init(params):
    # Initializing Adam state for kappa1
    return AdamState(jnp.array(0), jnp.zeros_like(params.kappa1, dtype=jnp.float32), jnp.zeros_like(params.kappa1, dtype=jnp.float32))

def adam_update(i, grad_k1, state, learning_rate=1e-3, b1=0.9, b2=0.999, eps=1e-8):
    lr_val = learning_rate

    state = state.replace(count=state.count + 1)
    i = state.count

    m = (b1 * state.m_k1) + (1 - b1) * grad_k1
    v = (b2 * state.v_k1) + (1 - b2) * (grad_k1 * grad_k1)

    m_hat = m / (1 - b1**i)
    v_hat = v / (1 - b2**i)

    delta_k1 = lr_val * m_hat / (jnp.sqrt(v_hat) + eps)

    new_state = AdamState(state.count, m, v)
    return new_state, delta_k1

# =============================================================================
# --- 1. CCZ4 STATE (PYTREE) & INITIALIZATION ---
# =============================================================================
@register_pytree_node_class
class CCZ4State:
    def __init__(self, phi, chi, K, At, Gam, Theta, Z, alpha, beta):
        self.phi, self.chi, self.K, self.At, self.Gam, self.Theta, self.Z, self.alpha, self.beta = \
            phi, chi, K, At, Gam, Theta, Z, alpha, beta
    def to_tuple(self):
        return (self.phi, self.chi, self.K, self.At, self.Gam, self.Theta, self.Z, self.alpha, self.beta)
    @classmethod
    def from_tuple(cls, t):
        return cls(*t)
    def tree_flatten(self):
        return self.to_tuple(), None
    @classmethod
    def tree_unflatten(cls, aux_data, children):
        return cls(*children)

def initialize_minkowski(N):
    zeros = jnp.zeros(FIELD_SHAPE, dtype=jnp.float32)
    ones = jnp.ones(FIELD_SHAPE, dtype=jnp.float32)
    r = jnp.linalg.norm(jnp.mgrid[:N,:N,:N] - N/2, axis=0) * dx
    r = jnp.asarray(r, dtype=jnp.float32)
    alpha_init = ones - 0.01 * jnp.exp(-r**2 / 5.0)
    # Small perturbation in K (Trace K)
    perturbation = 1e-2 * jnp.exp(-r**2 / 5.0)
    K_init = zeros + perturbation
    return CCZ4State(zeros, ones, K_init, jnp.zeros((5,) + FIELD_SHAPE, dtype=jnp.float32),
                     jnp.zeros((3,) + FIELD_SHAPE, dtype=jnp.float32), zeros, jnp.zeros((3,) + FIELD_SHAPE, dtype=jnp.float32),
                     alpha_init, jnp.zeros((3,) + FIELD_SHAPE, dtype=jnp.float32))

# =============================================================================
# --- 2. TENSOR ALGEBRA & CONSTRAINTS ---
# =============================================================================
def diff6(f, dx):
    d_x = jnp.roll(f, -1, axis=0) - jnp.roll(f, 1, axis=0)
    d_y = jnp.roll(f, -1, axis=1) - jnp.roll(f, 1, axis=1)
    d_z = jnp.roll(f, -1, axis=2) - jnp.roll(f, 1, axis=2)
    # Standard 6th order finite difference (2nd order here for simplicity)
    return jnp.stack([d_x, d_y, d_z], axis=0) / (2.0 * dx)
def grad6(f): return diff6(f, dx)
def advect(f, beta, d):
    grad_f = d(f)
    return beta[0]*grad_f[0] + beta[1]*grad_f[1] + beta[2]*grad_f[2]

@jit
def voigt_to_tensor(A_voigt):
    shape = A_voigt.shape[1:]
    A_full = jnp.zeros((DIM, DIM) + shape, dtype=jnp.float32)
    A_full = A_full.at[0,0].set(A_voigt[0])
    A_full = A_full.at[1,1].set(A_voigt[3])
    A_full = A_full.at[2,2].set(-A_voigt[0] - A_voigt[3]) # Assuming trace-free property (or default)
    A_full = A_full.at[0,1].set(A_voigt[1]); A_full = A_full.at[1,0].set(A_voigt[1])
    A_full = A_full.at[0,2].set(A_voigt[2]); A_full = A_full.at[2,0].set(A_voigt[2])
    A_full = A_full.at[1,2].set(A_voigt[4]); A_full = A_full.at[2,1].set(A_voigt[4])
    return A_full

@jit
def get_gamma_inv(state):
    # Metric inversion for a flat metric (gamma_ij = delta_ij * chi)
    return jnp.identity(DIM, dtype=jnp.float32)[:, :, None, None, None] * jnp.ones((DIM, DIM) + FIELD_SHAPE, dtype=jnp.float32)

@jit
def hamiltonian_constraint(state):
    chi = state.chi
    At_full = voigt_to_tensor(state.At)
    gamma_inv = get_gamma_inv(state)
    # A^2 = A^ij A_ij
    At_sq = jnp.einsum('ij..., kl..., ik..., jl... -> ...',
                       At_full, At_full, gamma_inv, gamma_inv)
    log_chi = jnp.log(chi + 1e-15)
    d_log_chi = grad6(log_chi)
    grad_sq_sum = jnp.sum(d_log_chi * d_log_chi, axis=0)
    R_chi = -8 * grad_sq_sum # Placeholder for the simplified Ricci scalar
    # Simplified H constraint
    H = (R_chi + state.K**2 - At_sq) / chi + 2.0 * state.K * state.Theta
    return H

@jit
def momentum_constraint(state):
    At_full = voigt_to_tensor(state.At)
    d_At = grad6(At_full)
    # M_i ~ D^j A_ij
    Mi = jnp.einsum('jk..., kij... -> i...',
                    get_gamma_inv(state), d_At)
    return Mi * 0.1 # Scaling factor for better gradient stability

@jit
def tensor_to_voigt_rhs(A_full):
    """Converts a (3, 3, N, N, N) tensor back to 5 Voigt components (5, N, N, N)."""
    return jnp.stack([
        A_full[0,0],
        A_full[0,1],
        A_full[0,2],
        A_full[1,1],
        A_full[1,2]
    ], axis=0)

# =============================================================================
# --- 3. CCZ4 RHS (MASTER EVOLUTION EQUATIONS) ---
# =============================================================================
def ccz4_rhs(state_tuple, dx, params):
    state = CCZ4State.from_tuple(state_tuple)
    d = lambda f: diff6(f, dx)
    Lie = lambda f: advect(f, state.beta, d)
    H = hamiltonian_constraint(state)
    Mi = momentum_constraint(state)

    alpha = state.alpha
    K = state.K
    At_full = voigt_to_tensor(state.At)

    # Dependency on params.kappa1 (Constraint Damping for Theta)
    dTheta_dt = (
        Lie(state.Theta)
        + 0.5 * alpha * H
        - alpha * (2.0 - params.kappa1) * K * state.Theta
        - params.kappa1 * alpha * state.Theta
    )
    # Dependency on params.kappa2 (Constraint Damping for Z)
    dZ_dt = (
        Lie(state.Z)
        + alpha * (Mi - (2/3) * d(K) * state.Theta)
        - params.kappa2 * alpha * state.Z
    )

    dphi_dt = jnp.zeros_like(state.phi)
    dchi_dt = Lie(state.chi) + (1.0 / 3.0) * state.chi * alpha * K
    dK_dt = (
        Lie(K)
        + alpha * (K**2 / 3.0 + H / 2.0)
        + alpha * 0.0 # Placeholder for non-linear terms
    )

    Ricc_proxy = K * K * jnp.identity(DIM, dtype=jnp.float32)[:, :, None, None, None] * 0.1
    dAt_dt = (
        Lie(At_full)
        - alpha * (Ricc_proxy - K * At_full)
        - 2.0 * alpha * K * At_full
    )

    dAt_dt_voigt = tensor_to_voigt_rhs(dAt_dt)

    # Dependency on params.eta (Shift Damping / Gamma-driver)
    dGam_dt = Lie(state.Gam) - params.eta * alpha * state.Gam + alpha * Mi
    dalpha_dt = -2.0 * alpha * K # 1+log Slicing
    dbeta_dt = 0.75 * state.Gam - params.eta * state.beta # Gamma-driver shift

    return (dphi_dt, dchi_dt, dK_dt, dAt_dt_voigt, dGam_dt, dTheta_dt, dZ_dt, dalpha_dt, dbeta_dt)


# =============================================================================
# --- 4. INTEGRATOR & STEP FUNCTIONS ---
# =============================================================================
def rk4_step(state_tuple, rhs_fn, dt, dx, params):

    def add_scaled(state, k, scale):
        return tree_map(lambda x, y: x + scale * y, state, k)

    dt_f = jnp.float32(dt)

    k1 = rhs_fn(state_tuple, dx, params)
    k2 = rhs_fn(add_scaled(state_tuple, k1, dt_f * 0.5), dx, params)
    k3 = rhs_fn(add_scaled(state_tuple, k2, dt_f * 0.5), dx, params)
    k4 = rhs_fn(add_scaled(state_tuple, k3, dt_f), dx, params)

    final_step = tree_map(lambda k1_c, k2_c, k3_c, k4_c: (dt_f / 6.0) * (k1_c + 2*k2_c + 2*k3_c + k4_c),
                          k1, k2, k3, k4)

    return tree_map(lambda s, f: s + f, state_tuple, final_step)

def ccz4_step(state_tuple, dt, dx, params):
    return rk4_step(state_tuple, ccz4_rhs, dt, dx, params)

# Crucial for memory management in JAX
checkpointed_step = checkpoint(ccz4_step)

# =============================================================================
# --- 5. LOOPING (EVOLUTION) ---
# =============================================================================
def raw_full_adjoint_evolution(initial_state_tuple, num_steps, dt, dx, params, checkpoint_interval):

    initial_loop_state = (initial_state_tuple, dt, params)

    def single_step_pytree(loop_state):
        state, dt_val, params_val = loop_state
        new_state = ccz4_step(state, dt_val, dx, params_val)
        return (new_state, dt_val, params_val)

    def checkpointed_step_pytree(loop_state):
        state, dt_val, params_val = loop_state
        new_state = checkpointed_step(state, dt_val, dx, params_val)
        return (new_state, dt_val, params_val)

    # Use checkpointing every N steps to save memory while allowing Autodiff through the loop
    def body(i, loop_state):
        return cond(i % checkpoint_interval == 0,
                    checkpointed_step_pytree,
                    single_step_pytree,
                    loop_state)

    final_loop_state = fori_loop(0, num_steps, body, initial_loop_state)
    final_state_tuple, _, _ = final_loop_state
    return final_state_tuple

# Currying and JIT compile the main evolution function
full_adjoint_evolution = partial(jit, static_argnames=('num_steps', 'dx', 'checkpoint_interval'))(raw_full_adjoint_evolution)

# =============================================================================
# --- 6. LOSS FUNCTION AND GRADIENT ---
# =============================================================================
def raw_loss_fn(initial_state_tuple, params, TEST_TIMESTEPS, GLOBAL_DT, dx, CHECKPOINT_INTERVAL):
    """Loss: minimizes Constraint violation (Theta, K, and Momentum)."""

    final = full_adjoint_evolution(initial_state_tuple, TEST_TIMESTEPS, GLOBAL_DT,
                                   dx, params, CHECKPOINT_INTERVAL)

    state = CCZ4State.from_tuple(final)
    # Sum of the squared momentum constraint violation
    Mi_sq = jnp.mean(momentum_constraint(state)**2)

    # Loss weight K^2 = 1.0 (Critical for optimization stability)
    return jnp.mean(state.Theta**2) + 1.0 * jnp.mean(state.K**2) + Mi_sq

# 1. Main Loss Function (curried and JITted)
jitted_loss_fn = jit(partial(raw_loss_fn, TEST_TIMESTEPS=TEST_TIMESTEPS, GLOBAL_DT=GLOBAL_DT,
                                          dx=dx, CHECKPOINT_INTERVAL=CHECKPOINT_INTERVAL))

# 2. Gradient w.r.t. the Params PyTree (argument index 1)
grad_params_raw = grad(lambda state, p: raw_loss_fn(state, p, TEST_TIMESTEPS, GLOBAL_DT, dx, CHECKPOINT_INTERVAL), argnums=1)

# 3. Wrapper to extract the kappa1 gradient specifically
def get_kappa1_gradient(initial_state_tuple, params):
    grad_ptree = grad_params_raw(initial_state_tuple, params)
    # Target the gradient for kappa1
    return grad_ptree.kappa1

# =============================================================================
# --- 7. GRADIENT DESCENT BENCHMARK ---
# =============================================================================

@jit
def optimization_step(current_params, current_state_tuple):

    grad_k1_raw = get_kappa1_gradient(current_state_tuple, current_params)
    grad_k1 = jnp.float32(grad_k1_raw)

    loss_val = jitted_loss_fn(current_params, current_state_tuple)
    return loss_val, grad_k1

def run_gradient_descent_benchmark():
    global N, dx, FIELD_SHAPE, CHECKPOINT_INTERVAL, TEST_TIMESTEPS, LEARNING_RATE, N_ITERATIONS
    N_start = N

    FIELD_SHAPE = (N, N, N)

    initial_state = initialize_minkowski(N)
    state_tuple = initial_state.to_tuple()

    # CRITICAL: Use an initial value of 0.0 to force convergence towards 2.0
    params_opt = PARAMS
    opt_state = adam_init(params_opt)

    print(f"--- üèÜ DIFF-CCZ4 GRADIENT DESCENT TEST N={N_start}^3 / T={TEST_TIMESTEPS} (K1_INIT: {PARAMS.kappa1.item():.1f}, K2: {PARAMS.kappa2.item():.1f}, ETA: {PARAMS.eta.item():.1f}, LR: {LEARNING_RATE:.0e}, ITER: {N_ITERATIONS}) ---")
    print(f"--- üéØ OPTIMIZING KAPPA1 (Target 2.0) ---")

    print("Compiling loss function...", end="")
    try:
        loss_val_dummy, grad_k1_dummy = optimization_step(params_opt, state_tuple)
        loss_val_dummy.block_until_ready()
        print("DONE")
    except Exception as e:
        print(f"ERROR: Compilation failed: {e}")
        return

    loss_history = []

    # MAIN OPTIMIZATION LOOP
    for step in range(1, N_ITERATIONS + 1):
        start_step = time.time()

        # The order of arguments here must match the @jit optimization_step definition
        loss_val, grad_k1 = optimization_step(params_opt, state_tuple)

        opt_state, delta_k1 = adam_update(step, grad_k1, opt_state, learning_rate=LEARNING_RATE)

        new_k1 = params_opt.kappa1 - delta_k1
        params_opt = params_opt.replace(kappa1=new_k1)

        loss_val.block_until_ready()
        step_time = time.time() - start_step
        loss_history.append(loss_val)

        if step % 50 == 0 or step <= 10:
            print(f"Step {step:03d} | Loss: {loss_val.item():.2e} | Grad K1: {grad_k1.item():.2e} | Time: {step_time:.2f}s | K1: {params_opt.kappa1.item():.4f}")

    print("\n--- ‚úÖ FINAL OPTIMIZATION RESULTS ---")
    print(f"Initial Loss (Step 1): {loss_history[0].item():.2e}")
    print(f"Final Loss (Step {N_ITERATIONS}): {loss_history[-1].item():.2e}")
    print(f"Final Kappa1 Value: {params_opt.kappa1.item():.4f}")

    if abs(params_opt.kappa1.item() - 2.0) < 0.1:
        print("STATUS: SUCCESS. Kappa1 converged successfully near the optimal value of 2.0.")
    elif loss_history[-1] < loss_history[0]:
        print("STATUS: SUCCESS. Loss decreased. K1 moved in the correct direction.")
    else:
        print("STATUS: CAUTION. Check if Loss or K1 moved in the wrong direction.")

if __name__ == "__main__":
    run_gradient_descent_benchmark()
